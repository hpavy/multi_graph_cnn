\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{GDAFinal_project}
\author{Valentin Exbrayat}
\date{December 2025}

\begin{document}

\maketitle

\section{Abstract}
Recommender systems are central to modern digital platforms, yet their underlying user–item rating matrices are highly sparse and structurally complex. While classical collaborative filtering and low-rank matrix completion methods struggle to capture the relational geometry of users and items, recent graph-regularized approaches exploit smoothness on similarity graphs but remain shallow and scale poorly. The emergence of geometric deep learning has introduced expressive tools such as spectral and Chebyshev graph convolutions, enabling localized, size-independent filtering on graph-structured data. Building on these developments, Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks (Monti et al., 2017) proposed a novel architecture that combines graph CNNs to extract locally stationary features and recurrent networks to model nonlinear rating diffusion. Although influential, this framework relies on static and fully accurate user/item graphs, lacks multi-scale geometric reasoning, and offers limited interpretability of its learned diffusion dynamics. These limitations motivate a re-examination of geometric approaches to matrix completion and form the basis of the present work.

\section{Context}
Recommender systems have become indispensable to modern digital platforms, providing personalized suggestions in environments defined by overwhelming choice and information overload. From e-commerce and multimedia platforms to news feeds and social networks, these systems infer user preferences from massive, sparse, and often noisy interaction data. The key challenge lies in predicting missing user–item interactions despite sparsity, heterogeneity, and evolving user behavior. Classical paradigms such as collaborative filtering and content-based approaches (Breese et al., 1998; Pazzani & Billsus, 2007) remain foundational but do not fully leverage the rich relational structure that exists among users and items—structure that is crucial for improving generalization in sparse settings. These limitations have motivated the emergence of methods that explicitly integrate geometric and structural priors into the recommendation process, a direction that culminated in the Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks framework introduced by Monti et al. (2017).

Before geometric methods, the dominant line of research was low-rank matrix completion (Candès & Recht, 2012), which models user preferences as points within a shared low-dimensional latent space. Although effective, this approach assumes that interactions depend purely on latent factors and does not incorporate known similarities between users or between items. To address this gap, graph-regularized matrix completion introduced user and item similarity graphs and enforced smoothness priors—encouraging neighboring nodes in each graph to exhibit similar rating patterns (Ma et al., 2011; Kalofolias et al., 2014; Rao et al., 2015). These methods brought geometric information into the problem but remained tied to linear models whose capacity is limited and whose parameterization scales with the number of users and items.

In parallel, the field of geometric deep learning opened the door to more expressive, nonlinear approaches capable of operating directly on irregular domains. Foundational developments in spectral graph theory demonstrated that convolution on graphs can be defined through the Fourier basis induced by the graph Laplacian (Bruna et al., 2013; Shuman et al., 2013). Building on this, Chebyshev polynomial filters (Defferrard et al., 2016) provided computationally efficient, localized, and scalable graph convolutions with parameter counts independent of graph size—an essential property for recommender systems with millions of users and items. These advances enabled models to process graph-structured data in a manner analogous to CNNs on regular grids, while remaining sensitive to the underlying geometry of user and item relationships.

It is within this convergence of graph-based modeling and deep learning that Monti et al. (2017) introduced Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks. Their central contribution lies in treating the user–item rating matrix as data living on two interacting graphs—one for users and one for items—and learning representations through localized graph convolutional filters. This geometric viewpoint allows the model to extract fine-grained, spatially smooth features reflecting how preferences vary along each graph. Coupled with a recurrent module designed to model rating propagation as a nonlinear diffusion process, the approach blends spatial geometric structure with temporal dynamics. The resulting architecture represents one of the earliest and most influential attempts to unify graph convolutional networks with recurrent neural models for matrix completion, offering a principled, parameter-efficient framework tailored to the relational nature of recommendation tasks.

\section{Main content}
\subsection{Theoretical Formulation}

In this section, we summarize the theoretical framework introduced in 
\cite{monti2017geometric} for geometric matrix completion with multi-graph 
neural networks. The goal is to reconstruct a partially observed matrix 
$Y \in \mathbb{R}^{m \times n}$ from entries indexed by 
$\Omega \subseteq [m] \times [n]$, while leveraging structural information 
encoded in graphs over rows and columns.

\subsubsection{Rank Minimization and Graph-Regularized Formulation}

The original problem is formulated as a rank minimization:
\begin{equation}
    \min_{X} \ \operatorname{rank}(X) 
    \quad \text{s.t. } x_{ij} = y_{ij}, \quad \forall (i,j) \in \Omega.
\end{equation}
Since rank minimization is NP-hard, the authors propose a convex surrogate 
incorporating smoothness constraints with respect to row and column graphs:
\begin{equation}
    \min_{X} \ \|X\|_{G_r}^2 + \|X\|_{G_c}^2 + \frac{\mu}{2} 
    \|\Omega \circ (X - Y)\|_F^2,
\end{equation}
where \(\|X\|_{G_r}^2 = \mathrm{tr}(X^\top \Delta_r X)\) and 
\(\|X\|_{G_c}^2 = \mathrm{tr}(X \Delta_c X^\top)\), with \(\Delta_r\) 
and \(\Delta_c\) being the Laplacians of the row and column graphs, 
respectively. This formulation encourages the reconstructed matrix to be 
smooth with respect to the underlying graphs.  

To explicitly enforce low-rank structure, the matrix is factorized as 
$X = WH^\top$, leading to
\begin{equation}
    \min_{W,H} \ \frac{1}{2}\|W\|_{G_r}^2 + \frac{1}{2}\|H\|_{G_c}^2 
    + \frac{\mu}{2} \|\Omega \circ (WH^\top - Y)\|_F^2,
\end{equation}
where \(W \in \mathbb{R}^{m \times r}\) and \(H \in \mathbb{R}^{n \times r}\).  
These regularization terms encourage the factors to vary smoothly along the 
respective graphs.  

\subsubsection{Spectral Graph Filtering}

The authors describe filtering along the graphs using the spectral domain.  
Let the Laplacians be decomposed as
\[
\Delta_r = \Phi_r \Lambda_r \Phi_r^\top, \qquad 
\Delta_c = \Phi_c \Lambda_c \Phi_c^\top.
\]
The graph Fourier transform of \(X\) is \(\widehat{X} = \Phi_r^\top X \Phi_c\).  
A spectral filter with multiplier \(\widehat{Y}\) acts element-wise on \(\widehat{X}\):
\begin{equation}
    X \star Y = \Phi_r (\widehat{X} \circ \widehat{Y}) \Phi_c^\top,
\end{equation}
where \(\circ\) denotes element-wise multiplication.  
This formulation is explicitly presented in the paper and highlights how 
the filtering operation combines information from row and column graph 
spectra.


\subsubsection{Chebyshev Polynomial Expansion and Parameter Interpretation}

While the paper does not derive the full equivalence, it explains that the 
spectral multiplier \(\widehat{Y}\) can be approximated using a truncated 
bivariate Chebyshev polynomial expansion:
\begin{equation}
\tau(\widetilde{\lambda}_r, \widetilde{\lambda}_c) 
= \sum_{p,q=0}^{P} \theta_{pq} \, T_p(\widetilde{\lambda}_r) \, T_q(\widetilde{\lambda}_c),
\end{equation}
where \(T_p\) denotes the Chebyshev polynomial of degree \(p\), and 
\(\Theta = (\theta_{pq})\) is the matrix of coefficients.  
Importantly, the paper explicitly mentions that truncating the expansion 
reduces the number of parameters from \(O(mn)\) (the full spectral multiplier) 
to \((P+1)^2\), making the approach computationally tractable.

This expansion expresses the spectral filter as a polynomial of the 
eigenvalues of the row and column Laplacians, showing that the convolution 
depends only on the spectra and not on the eigenvectors themselves.   
Intuitively, each coefficient $\theta_{pq}$ controls the amplitude assigned to a 
specific pair of graph frequencies: the $p$-th frequency of the row graph and the 
$q$-th frequency of the column graph. This follows from the fact that the graph 
Fourier transform diagonalizes the Laplacian, so that the eigenvectors 
$\Phi_r$ and $\Phi_c$ define the orthogonal basis in which any matrix $X$ can be 
decomposed into its elementary oscillatory components. In this spectral domain, 
the eigenvalues $\widetilde{\lambda}_{r,i}$ and $\widetilde{\lambda}_{c,j}$ 
represent the corresponding frequencies: small eigenvalues encode smooth 
variations along the graph, whereas large eigenvalues encode rapid variations.  

Because convolution is defined as a multiplication in the spectral domain, its 
effect is entirely determined by how much we amplify or attenuate each spectral 
component. The crucial observation is that this amplification must depend only 
on the frequencies of the signal, not its orientation in the original 
vertex domain; hence the convolution operator seems fully described by the scalar 
function $\tau(\widetilde{\lambda}_{r,i},\widetilde{\lambda}_{c,j})$, evaluated 
at pairs of eigenvalues. In other words, once the Laplacian eigenvectors define 
the notion of frequency on the graph, the eigenvalues alone determine how each 
frequency component is filtered.  

The Chebyshev expansion 
\[
\tau(\widetilde{\lambda}_r,\widetilde{\lambda}_c)
= \sum_{p,q=0}^P \theta_{pq}\,
T_p(\widetilde{\lambda}_r)\, T_q(\widetilde{\lambda}_c)
\]
therefore encodes how the filter behaves across all frequency pairs 
$(\widetilde{\lambda}_r,\widetilde{\lambda}_c)$. Each term 
$T_p(\widetilde{\lambda}_r) T_q(\widetilde{\lambda}_c)$ prescribes a specific 
pattern of response over the joint frequency plane of the two graphs, and the 
learned parameters $\theta_{pq}$ modulate the contribution of these patterns. 
Since Chebyshev polynomials form an approximation basis over $[-1,1]$, this 
representation allows the model to approximate any smooth spectral filter using 
only $(P+1)^2$ parameters.  


### maybe we can let this part away and interprret the equation just below TOM t'en penses quoi ?
From this perspective, using eigenvalues for convolution naturally captures the 
dominant tendencies of variation on the graphs: low-frequency components (small 
eigenvalues) correspond to broad, smooth structures shared by many vertices, 
whereas high-frequency components correspond to localized or noisy fluctuations. 
The spectral filter can thus selectively enhance global trends or suppress noise 
simply by assigning appropriate weights to different ranges of eigenvalues, 
making the convolution both expressive and interpretable.

Using standard functional calculus, one can see that this spectral 
multiplication is equivalent to polynomial filtering in the vertex domain:
\begin{equation}
    \widetilde{X} = \sum_{p,q=0}^{P} \theta_{pq} \, 
    T_p(\widetilde{\Delta}_r) \, X \, T_q(\widetilde{\Delta}_c),
\end{equation}
which corresponds to Equation (12).  
This equivalence shows that the filtering operation can be performed efficiently 
without explicit eigenvector computations, while preserving the dependency 
on the graph spectra via the Chebyshev polynomials.


\subsubsection{Recurrent Neural Network Component}

The framework further introduces a recurrent neural network that operates 
on these filtered signals. At each step, the RNN updates the state by combining 
the current filtered matrix with previous states, enabling propagation of 
information across multiple graph supports and modeling nonlinear interactions.

\subsection{Implementations}


\subsection{Transition}

Beyond the original presentation, several limitations can be noted.  
First, the method relies on pre-constructed row and column graphs, which 
are assumed static and accurate, potentially limiting adaptability to 
dynamic or noisy data. Second, the graph filters are local, focusing on 
stationarity at small neighborhoods, without an explicit mechanism for 
capturing global or multi-scale graph patterns. Finally, while the recurrent 
module introduces nonlinearity and temporal propagation, the learned dynamics 
are not easily interpretable, leaving the relationship between filtered 
signals and the underlying rating diffusion unclear.


\section{Showing the learnt embedding}

\subsection{The idea}

From our previous sections, we have seen that the model is combining two graphs and a ranking matrix in order to do a prediction. From the part focusing on the influence of the graph, we know that the model is able to take only what is interesting for it in order to do the prediction. If the graph is good for that, it is going to use it. Otherwise it will learn to do without. \\

From that, we can see the model as a tool in order to create a new embedding for the users. This embedding will be especially good for ranking prediction for the task we want to solve. \\

Our goal in this part is to study this embedding. The best embedding should be the one at the end of the convolution. On top of that, the use of the matrices $W$ and $H$ from the factorized model (part 2.1 of the paper) give us directly a reduced embedding for the users or movies (if we take the example of users ranking movies). \\


\subsection{The experiment}
In order to show that the information inside the embedding is better than the one in the ranking or the graph alone, we are going to retake the synthetic dataset (and the example of movies and users for the understanding). The goal will be to create two clusters, then create a graph from those users clusters. 

We are going to take a bad value for $p_{within}$ such that the clustering with the graph is not perfect. Then our goal is from the user embedding, to do a clustering and to try to refind the two clusters with a good accuracy. 


